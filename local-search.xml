<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>logistic_classification</title>
    <link href="/2020/01/11/logistic-classification/"/>
    <url>/2020/01/11/logistic-classification/</url>
    
    <content type="html"><![CDATA[<h1 id="逻辑斯蒂-Logistic-回归—easy二分类"><a href="#逻辑斯蒂-Logistic-回归—easy二分类" class="headerlink" title="逻辑斯蒂(Logistic)回归—easy二分类"></a>逻辑斯蒂(Logistic)回归—easy二分类</h1><h2 id="1-根据stimoid函数-0-1分布"><a href="#1-根据stimoid函数-0-1分布" class="headerlink" title="1.根据stimoid函数(0-1分布)"></a>1.根据stimoid函数(0-1分布)</h2><p>​     P(y=1|x;$\theta$)=$h_{\theta}(x)$        P(y=0|x;$\theta$)=1-$h_{\theta}(x)$</p><h2 id="2-loss-function"><a href="#2-loss-function" class="headerlink" title="2.loss function"></a>2.loss function</h2><p>​      loss($h_{\theta}$,y)=$h_{\theta}(x)^y$+$(1-h_{\theta}(x))^{1-y}$    //二分类loss func</p><p>y=1/0  等于1 时<strong>loss($h_{\theta}$,y)=$h_{\theta}(x)$=  P(y=1|x;$\theta$)</strong></p><p>​             等于0 时 <strong>loss($h_{\theta}$,y)=$1-h_{\theta}(x)$=P(y=0|x;$\theta$)</strong></p><p>nodes： loss func ==y 为 1/0 的概率             <em>*</em></p><p>取log</p><p>​        loss($h_{\theta}$,y)=ylog($h_{\theta}(x)$)+(1-y)log($(1-h_{\theta}(x))$)</p><p> <strong>最大似然估计</strong>（sample 之间相互独立）<strong>概率连乘</strong>得到整体的loss func</p><p>​        J($\theta$)=$\sum_{i=1}^{m} (y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))$</p><p>此时根据(<em>*</em>)的内容， 可以通过这个损失函数求出样本所属类别的概率，而这个概率越大越好，则此时求的时J($\theta$) MAX VALUE。</p><p>计算J($\theta$) MAX VALUE的方法：梯度上升法</p><p>   同梯度下降的原理相同，但与其形式不同：</p><p>​                         $\theta$=$\theta$+$\alpha \frac{\bigtriangledown j(\theta)}{\theta}$</p><p>问题转换为如何求J($\theta$) 对$\theta$ 的偏导,由 $h_{\theta}(x)$</p><p>​           $h_{\theta}(x)$ =g($\theta^{T}x$)=$\frac{1}{1+e^{-\theta^{T}x}}$</p><p>对$\theta$ 求偏导(隐函数求偏导—》偏导数连乘)</p><p>​     $\frac{J(\partial)}{\theta_j}J({\theta})$=$\frac{\partial J(\theta)}{\partial g(\theta^{T}x)}$ <em>$\frac{\partial g(\theta^{T}x) }{\partial \theta^{T}x}$ </em>$\frac{\partial \theta^Tx}{\partial \theta_j}$</p><p> 推导得：</p><p>​              $\theta_j:=\theta_j+\alpha \sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}$</p><p>自此梯度上升迭代公式推到完毕</p><p>梯度上升Python代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradAscent</span><span class="hljs-params">(dataMatIn, classLabels)</span>:</span><br>    dataMatrix=np.mat(dataMatIn)<br>    labelMat=np.mat(classLabels).transpose()<br>    m,n=np.shape(dataMatrix)<br>    alpha=<span class="hljs-number">0.01</span> <span class="hljs-comment">#学习步长</span><br>    cycleNum=<span class="hljs-number">500</span> <span class="hljs-comment">#迭代次数</span><br>    weight=np.ones([n,<span class="hljs-number">1</span>]) <span class="hljs-comment">#初始化权重</span><br>    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(cycleNum):<br>        h=sigmoid(dataMatrix*weight)<br>        error=labelMat-h<br>        weight=weight+alpha*dataMatrix.transpose()*error<br>    <span class="hljs-keyword">return</span> weight.getA()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>machine learning logisticClassification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>recommedationSY</title>
    <link href="/2019/12/29/recommedationSY/"/>
    <url>/2019/12/29/recommedationSY/</url>
    
    <content type="html"><![CDATA[<h1 id="基于内容推荐的的-cost-function"><a href="#基于内容推荐的的-cost-function" class="headerlink" title="基于内容推荐的的 cost function"></a>基于内容推荐的的 cost function</h1><p>condition：电影内容($x^i$)&amp;&amp;用户评分表$y^ (i,j)$=&gt;用户喜好($\Theta^j$)</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><p>condition：电影内容($x^i$)&amp;&amp;用户评分表$y^ (i,j)$=&gt;用户喜好($\Theta^j$)</p><p><img src="https://i.loli.net/2019/12/29/UXbfzq2SeM1FdKD.png" srcset="/img/loading.gif" alt="屏幕截图_3_.png"></p><script type="math/tex; mode=display">r(i,j) 指的是j用户对编号为i的电影是否评分   \\ x^i 指的是电影的类型评分                   \\ 正则项避免过拟合  \\ \Theta^j  为j用户的喜好</script><h1 id="协同过滤-只需要用户对电影的评分"><a href="#协同过滤-只需要用户对电影的评分" class="headerlink" title="协同过滤:只需要用户对电影的评分"></a><strong>协同过滤</strong>:只需要用户对电影的评分</h1><p>condition：用户电影评分表($y^ (i,j)$)&amp;&amp;用户喜好($\Theta ^j$)=&gt; 电影内容($x^i$)</p><p><img src="https://i.loli.net/2019/12/29/rDzWUTmAcF6d5MR.png" srcset="/img/loading.gif" alt="屏幕截图_5_.png"></p><p><strong>整合后的loss function：</strong></p><p><img src="https://i.loli.net/2019/12/29/dQ4pLjOotEaNPux.png" srcset="/img/loading.gif" alt="屏幕截图_6_.png"></p><h1 id="基于item的协同过滤：计算item之间的相似度，依据用户对于相似item的评分推测用户对于本item的评分"><a href="#基于item的协同过滤：计算item之间的相似度，依据用户对于相似item的评分推测用户对于本item的评分" class="headerlink" title="基于item的协同过滤：计算item之间的相似度，依据用户对于相似item的评分推测用户对于本item的评分"></a>基于item的协同过滤：计算item之间的相似度，依据用户对于相似item的评分推测用户对于本item的评分</h1><p>cal similarity:</p><p><img src="https://i.loli.net/2019/12/29/wqaVNSIjnZlgbJc.png" srcset="/img/loading.gif" alt="屏幕截图_7_.png"></p><p><strong>通过similarity 计算评分：</strong></p><p><img src="https://i.loli.net/2019/12/29/szx7Adl2OHCf6qp.png" srcset="/img/loading.gif" alt="屏幕截图_8_.png"></p><h1 id="基于用户-USER-的协同过滤："><a href="#基于用户-USER-的协同过滤：" class="headerlink" title="基于用户(USER)的协同过滤："></a>基于用户(USER)的协同过滤：</h1><p>用户之间的相似度计算:</p><p><img src="https://i.loli.net/2019/12/29/iyBX3MYf2HIPrN9.png" srcset="/img/loading.gif" alt="屏幕截图_9_.png"></p><p>基于用户相似度就算本用户的预测分数：</p><p><img src="https://i.loli.net/2019/12/29/CarcO3w17jPHQb2.png" srcset="/img/loading.gif" alt="屏幕截图_10_.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>推荐系统 TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2019/12/28/hello-world/"/>
    <url>/2019/12/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>梯度下降</title>
    <link href="/2019/04/05/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <url>/2019/04/05/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    
    <content type="html"><![CDATA[<blockquote><p>高数复习了方向导数和梯度，并且方向导数沿着梯度的方向最大。</p></blockquote><p>据定义：在某一处的梯度是一个向量，设u=f(x,y,z)，则在（x,y,z）处的梯度</p><script type="math/tex; mode=display">grad \quad u| p_{0}(u_{x}^{'},u_{y}^{'},u_{z}^{'})</script><p>也就是说，某一处对x、y…自变量偏导的大小和方向决定了此处梯度的大小和方向，例如：对x的偏导指的是将其他自变量看作是constant（常量）时函数f(x,y,z)的导数，指的是f(x,y,z)在自变量某变化区间内的变化率。所以，梯度在x轴的分量是沿某x区间内的f(x,y,z)变化率递增的方向，</p><blockquote><p>梯度下降，指的就是沿着梯度的反方向（f(x,y,z)增长的反方向）反复下降以求局部最小值。</p></blockquote><p>梯度下降公式：</p><script type="math/tex; mode=display">j_{\theta}=j- \alpha cost</script><p>cost 指的是代价函数，在不同的模型选择上，cost的计算方式是不一样的。</p><p>比如逻辑回归、线性回归之间的代价函数就不一样。</p><script type="math/tex; mode=display">\alpha 指的是 学习步长，李宏毅老师叫learning\quad rate</script><p>学习步长的选取现在还不清楚</p>]]></content>
    
    
    <categories>
      
      <category>ml</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习基础知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
